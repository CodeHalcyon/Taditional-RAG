{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc0f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "loader  = PyMuPDFLoader(\n",
    "    \"cc2.pdf\",\n",
    ")\n",
    "doc = loader.load()\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0a215d",
   "metadata": {},
   "source": [
    "CREATING THE CHUNKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000726ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "    )\n",
    "chunks = text_splitter.split_documents(doc)\n",
    "chunk_content = []\n",
    "for chunk in chunks:\n",
    "    chunk_content.append(chunk.page_content)\n",
    "len(chunk_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5c3c4",
   "metadata": {},
   "source": [
    "IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3585b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import uuid\n",
    "from typing import Any, List, Dict, Tuple\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673bf750",
   "metadata": {},
   "source": [
    "MAKING THE EMBEDDING MANAGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3445a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"model loaded successfully: {self.model_name}\\n Embedding dimensions: {self.model.get_sentence_embedding_dimension}\")\n",
    "        except Exception as e:\n",
    "            print(\"error: \",e)\n",
    "            raise\n",
    "    def generate_embeddings(self, texts: List[str])-> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"model not loaded bitch!\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1a761",
   "metadata": {},
   "source": [
    "CREATING THE VECTOR STORAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStorage:\n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"./data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize()\n",
    "        \n",
    "    def _initialize(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory) #client have reference to the vector store chromadb\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"desc\":\"cc unit 3 pdf\"}\n",
    "            )\n",
    "            print(\"Vector store initialized: \", self.collection_name)\n",
    "            print(\"existing docs in collections: \", self.collection.count())\n",
    "        except Exception as e:\n",
    "            print(\"error setting up vector store: \", e)\n",
    "            raise\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        if len(documents)!= len(embeddings):\n",
    "            raise ValueError(\"Number of docs doesnt match embedding size\")\n",
    "        ids=[]\n",
    "        metadatas = []\n",
    "        doc_text = []\n",
    "        embed_list = []\n",
    "        \n",
    "        for i, (doc,embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata[\"doc_index\"] = i\n",
    "            metadata[\"content_length\"] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            doc_text.append(doc.page_content)\n",
    "            \n",
    "            embed_list.append(embedding.tolist())\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embed_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=doc_text\n",
    "            )\n",
    "            print(f\"added {len(documents)} to the vector store\")\n",
    "            print(\"total number of collections: \", self.collection.count())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"some error: \", e)\n",
    "            raise\n",
    "\n",
    "vector_store  = VectorStorage()\n",
    "vector_store\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afffa1d7",
   "metadata": {},
   "source": [
    "GENERATING THE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3123460",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_manager = EmbeddingManager()\n",
    "embed_manager._load_model()\n",
    "embeddings = embed_manager.generate_embeddings(chunk_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3fb18",
   "metadata": {},
   "source": [
    "ADDING DOCUMENTS INTO THE VECTOR STORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a1deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.add_documents(doc, embeddings)\n",
    "# len(embeddings)\n",
    "vector_store.add_documents(chunks, embeddings)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb8299",
   "metadata": {},
   "source": [
    "CREATING THE RETIVAL PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b94fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriver pipeline\n",
    "\n",
    "class ragretriver:\n",
    "    def __init__(self, vector_store: VectorStorage, embed_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embed_manager = embed_manager\n",
    "        \n",
    "    def retrieve(self, query: str, top_k: int =5, score_threshold: float=0.0)-> List[Dict[str,Any]]:\n",
    "        print(\"Retrieving document for the query: \", query)\n",
    "        query_emdedding = self.embed_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        try:\n",
    "            res = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_emdedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if res['documents'] and res['documents'][0]:\n",
    "                doc = res['documents'][0]\n",
    "                metadatas = res['metadatas'][0]\n",
    "                distances = res['distances'][0]\n",
    "                ids = res['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, doc, metadatas, distances)):\n",
    "                    similarity_score = 1-distance\n",
    "                    \n",
    "                    if similarity_score>=score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id':doc_id,\n",
    "                            'content': doc,\n",
    "                            'metadata': metadata,\n",
    "                            'similarit_score': similarity_score,\n",
    "                            'distance':distance,\n",
    "                            'rank':i+1\n",
    "                        }\n",
    "                        )\n",
    "                print(f\"{len(retrieved_docs)} documents fetched!\")\n",
    "                # print(retrieved_docs)\n",
    "            else:\n",
    "                print(\"No document matched\")\n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            print(\"some error:\")\n",
    "            raise\n",
    "retriver = ragretriver(vector_store, embed_manager)\n",
    "ans = retriver.retrieve(\"briefly explain the Characteristics of PaaS\")\n",
    "# for a in ans:\n",
    "#     print(a)\n",
    "''.join(ans[0]['content']).replace('\\n','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b64137a",
   "metadata": {},
   "source": [
    "CONENCTING TO A LLM FOR ENHANCING THE CONTEXT RESPONSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cddf92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model = 'gemini-2.5-flash',\n",
    "    temprature=0,\n",
    "    max_token = 1024\n",
    ")\n",
    "\n",
    "def rag_res(query, retriver, llm, top_k=3):\n",
    "    \n",
    "    # retrieve the context\n",
    "    res = retriver.retrieve(query, top_k=top_k)\n",
    "    context = ''.join(res[0]['content']).replace('\\n','') if res else \"\"\n",
    "    if not context:\n",
    "        return f\"no relevant context to the query: {query}\"\n",
    "    prompt = \"use the below context to answer the query preciesly: {context} and thew query is: {query}. just give straight your response no need to add your statements and also dont be like according to the context provided and all such opening statements\"\n",
    "    \n",
    "    res = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    return res.content\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da607a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = rag_res(\"\", retriver, llm)\n",
    "ans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
